{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def load_and_process_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and process MTA Congestion Relief Zone data with comprehensive cleaning and feature engineering\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert date and time columns to appropriate types\n",
    "    date_columns = ['Toll Date', 'Toll Week']\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "    timestamp_columns = ['Toll Hour', 'Toll 10 Minute Block']\n",
    "    for col in timestamp_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "    # Feature engineering - create useful derived columns\n",
    "    if 'Toll Date' in df.columns:\n",
    "        # Extract date components for easier filtering and aggregation\n",
    "        df['Year'] = df['Toll Date'].dt.year\n",
    "        df['Month'] = df['Toll Date'].dt.month\n",
    "        df['Month_Name'] = df['Toll Date'].dt.strftime('%B')\n",
    "        df['Week_Number'] = df['Toll Date'].dt.isocalendar().week\n",
    "        df['Is_Weekend'] = df['Day of Week Int'].apply(lambda x: 1 if x in [1, 7] else 0)\n",
    "        df['Is_Peak'] = df['Time Period'].apply(lambda x: 1 if x == 'Peak' else 0)\n",
    "    \n",
    "    # Clean data - handle missing values\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            df[col] = df[col].fillna(0)\n",
    "        else:\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "    \n",
    "    # Create aggregate views for common queries\n",
    "    print(f\"Creating aggregate views...\")\n",
    "    \n",
    "    # Daily aggregates\n",
    "    daily_entries = df.groupby('Toll Date').agg({\n",
    "        'CRZ Entries': 'sum',\n",
    "        'Excluded Roadway Entries': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Hourly aggregates by day of week\n",
    "    hourly_dow_entries = df.groupby(['Day of Week', 'Hour of Day']).agg({\n",
    "        'CRZ Entries': 'sum',\n",
    "        'Excluded Roadway Entries': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Vehicle class aggregates\n",
    "    vehicle_class_entries = df.groupby('Vehicle Class').agg({\n",
    "        'CRZ Entries': 'sum',\n",
    "        'Excluded Roadway Entries': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Entry point aggregates\n",
    "    entry_point_entries = df.groupby('Detection Group').agg({\n",
    "        'CRZ Entries': 'sum',\n",
    "        'Excluded Roadway Entries': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    aggregations = {\n",
    "        'daily': daily_entries,\n",
    "        'hourly_dow': hourly_dow_entries,\n",
    "        'vehicle_class': vehicle_class_entries,\n",
    "        'entry_point': entry_point_entries\n",
    "    }\n",
    "    \n",
    "    print(f\"Processed {df.shape[0]} records with {df.shape[1]} features\")\n",
    "    return df, aggregations\n",
    "\n",
    "def get_time_based_stats(df):\n",
    "    \"\"\"\n",
    "    Calculate common time-based statistics from the dataset\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'total_entries': int(df['CRZ Entries'].sum() + df['Excluded Roadway Entries'].sum()),\n",
    "        'crz_entries': int(df['CRZ Entries'].sum()),\n",
    "        'excluded_entries': int(df['Excluded Roadway Entries'].sum()),\n",
    "        'peak_hour': df.groupby('Hour of Day')['CRZ Entries'].sum().idxmax(),\n",
    "        'peak_day': df.groupby('Day of Week')['CRZ Entries'].sum().idxmax(),\n",
    "        'avg_daily_entries': int(df.groupby('Toll Date')['CRZ Entries'].sum().mean()),\n",
    "        'peak_percentage': float(df[df['Time Period'] == 'Peak']['CRZ Entries'].sum() / df['CRZ Entries'].sum() * 100),\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "def get_vehicle_stats(df):\n",
    "    \"\"\"\n",
    "    Calculate vehicle type statistics from the dataset\n",
    "    \"\"\"\n",
    "    # Most common vehicle type\n",
    "    vehicle_counts = df.groupby('Vehicle Class')['CRZ Entries'].sum()\n",
    "    most_common = vehicle_counts.idxmax()\n",
    "    most_common_count = vehicle_counts.max()\n",
    "    most_common_pct = most_common_count / vehicle_counts.sum() * 100\n",
    "    \n",
    "    stats = {\n",
    "        'most_common_vehicle': most_common,\n",
    "        'most_common_count': int(most_common_count),\n",
    "        'most_common_pct': float(most_common_pct),\n",
    "        'vehicle_distribution': dict(vehicle_counts / vehicle_counts.sum() * 100)\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "def get_location_stats(df):\n",
    "    \"\"\"\n",
    "    Calculate location-based statistics from the dataset\n",
    "    \"\"\"\n",
    "    # Busiest entry point\n",
    "    location_counts = df.groupby('Detection Group')['CRZ Entries'].sum()\n",
    "    busiest_entry = location_counts.idxmax()\n",
    "    busiest_entry_count = location_counts.max()\n",
    "    busiest_entry_pct = busiest_entry_count / location_counts.sum() * 100\n",
    "    \n",
    "    stats = {\n",
    "        'busiest_entry': busiest_entry,\n",
    "        'busiest_entry_count': int(busiest_entry_count),\n",
    "        'busiest_entry_pct': float(busiest_entry_pct),\n",
    "        'entry_distribution': dict(location_counts / location_counts.sum() * 100)\n",
    "    }\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/MTA_Congestion_Relief_Zone_Vehicle_Entries__Beginning_2025_20250404.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/g4v9z1k94wjb36xw4msmqb5m0000gn/T/ipykernel_45326/137913173.py:22: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n",
      "/var/folders/14/g4v9z1k94wjb36xw4msmqb5m0000gn/T/ipykernel_45326/137913173.py:22: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating aggregate views...\n",
      "Processed 870912 records with 20 features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Toll Date</th>\n",
       "      <th>Toll Hour</th>\n",
       "      <th>Toll 10 Minute Block</th>\n",
       "      <th>Minute of Hour</th>\n",
       "      <th>Hour of Day</th>\n",
       "      <th>Day of Week Int</th>\n",
       "      <th>Day of Week</th>\n",
       "      <th>Toll Week</th>\n",
       "      <th>Time Period</th>\n",
       "      <th>Vehicle Class</th>\n",
       "      <th>Detection Group</th>\n",
       "      <th>Detection Region</th>\n",
       "      <th>CRZ Entries</th>\n",
       "      <th>Excluded Roadway Entries</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Month_Name</th>\n",
       "      <th>Week_Number</th>\n",
       "      <th>Is_Weekend</th>\n",
       "      <th>Is_Peak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-29</td>\n",
       "      <td>2025-03-29 23:00:00</td>\n",
       "      <td>2025-03-29 23:50:00</td>\n",
       "      <td>50</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2025-03-23</td>\n",
       "      <td>Overnight</td>\n",
       "      <td>1 - Cars, Pickups and Vans</td>\n",
       "      <td>Brooklyn Bridge</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>103</td>\n",
       "      <td>99</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>March</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-29</td>\n",
       "      <td>2025-03-29 23:00:00</td>\n",
       "      <td>2025-03-29 23:50:00</td>\n",
       "      <td>50</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2025-03-23</td>\n",
       "      <td>Overnight</td>\n",
       "      <td>TLC Taxi/FHV</td>\n",
       "      <td>West Side Highway at 60th St</td>\n",
       "      <td>West Side Highway</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>March</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-29</td>\n",
       "      <td>2025-03-29 23:00:00</td>\n",
       "      <td>2025-03-29 23:50:00</td>\n",
       "      <td>50</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2025-03-23</td>\n",
       "      <td>Overnight</td>\n",
       "      <td>TLC Taxi/FHV</td>\n",
       "      <td>West 60th St</td>\n",
       "      <td>West 60th St</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>March</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-29</td>\n",
       "      <td>2025-03-29 23:00:00</td>\n",
       "      <td>2025-03-29 23:50:00</td>\n",
       "      <td>50</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2025-03-23</td>\n",
       "      <td>Overnight</td>\n",
       "      <td>TLC Taxi/FHV</td>\n",
       "      <td>Queensboro Bridge</td>\n",
       "      <td>Queens</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>March</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-29</td>\n",
       "      <td>2025-03-29 23:00:00</td>\n",
       "      <td>2025-03-29 23:50:00</td>\n",
       "      <td>50</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2025-03-23</td>\n",
       "      <td>Overnight</td>\n",
       "      <td>TLC Taxi/FHV</td>\n",
       "      <td>Queens Midtown Tunnel</td>\n",
       "      <td>Queens</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>March</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Toll Date           Toll Hour Toll 10 Minute Block  Minute of Hour  \\\n",
       "0 2025-03-29 2025-03-29 23:00:00  2025-03-29 23:50:00              50   \n",
       "1 2025-03-29 2025-03-29 23:00:00  2025-03-29 23:50:00              50   \n",
       "2 2025-03-29 2025-03-29 23:00:00  2025-03-29 23:50:00              50   \n",
       "3 2025-03-29 2025-03-29 23:00:00  2025-03-29 23:50:00              50   \n",
       "4 2025-03-29 2025-03-29 23:00:00  2025-03-29 23:50:00              50   \n",
       "\n",
       "   Hour of Day  Day of Week Int Day of Week  Toll Week Time Period  \\\n",
       "0           23                7    Saturday 2025-03-23   Overnight   \n",
       "1           23                7    Saturday 2025-03-23   Overnight   \n",
       "2           23                7    Saturday 2025-03-23   Overnight   \n",
       "3           23                7    Saturday 2025-03-23   Overnight   \n",
       "4           23                7    Saturday 2025-03-23   Overnight   \n",
       "\n",
       "                Vehicle Class               Detection Group  \\\n",
       "0  1 - Cars, Pickups and Vans               Brooklyn Bridge   \n",
       "1                TLC Taxi/FHV  West Side Highway at 60th St   \n",
       "2                TLC Taxi/FHV                  West 60th St   \n",
       "3                TLC Taxi/FHV             Queensboro Bridge   \n",
       "4                TLC Taxi/FHV         Queens Midtown Tunnel   \n",
       "\n",
       "    Detection Region  CRZ Entries  Excluded Roadway Entries  Year  Month  \\\n",
       "0           Brooklyn          103                        99  2025      3   \n",
       "1  West Side Highway           97                         2  2025      3   \n",
       "2       West 60th St          197                         0  2025      3   \n",
       "3             Queens           77                         0  2025      3   \n",
       "4             Queens          137                         0  2025      3   \n",
       "\n",
       "  Month_Name  Week_Number  Is_Weekend  Is_Peak  \n",
       "0      March           13           1        0  \n",
       "1      March           13           1        0  \n",
       "2      March           13           1        0  \n",
       "3      March           13           1        0  \n",
       "4      March           13           1        0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"data/MTA_Congestion_Relief_Zone_Vehicle_Entries__Beginning_2025_20250404.csv\"\n",
    "df_mta, aggregations = load_and_process_data(file_path)\n",
    "df_mta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_crz_data(df, \n",
    "                   start_date=None, \n",
    "                   end_date=None, \n",
    "                   day_type=None,  # 'weekday', 'weekend', or specific day name\n",
    "                   hour_range=None,  # tuple (start_hour, end_hour)\n",
    "                   time_period=None,  # 'Peak' or 'Overnight'\n",
    "                   vehicle_class=None,  # int or string for vehicle class\n",
    "                   entry_point=None,  # specific entry point\n",
    "                   entry_region=None):  # specific region\n",
    "    \"\"\"\n",
    "    Filter the CRZ dataset by multiple parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The MTA CRZ dataset\n",
    "    start_date : str, optional\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    end_date : str, optional\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    day_type : str, optional\n",
    "        'weekday', 'weekend', or specific day name (e.g., 'Monday')\n",
    "    hour_range : tuple, optional\n",
    "        (start_hour, end_hour) - integers from 0-23\n",
    "    time_period : str, optional\n",
    "        'Peak' or 'Overnight'\n",
    "    vehicle_class : str or int, optional\n",
    "        Vehicle class (1-5 or 'TLC Taxi/FHV')\n",
    "    entry_point : str, optional\n",
    "        Specific entry point (Detection Group)\n",
    "    entry_region : str, optional\n",
    "        Specific region (Detection Region)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Filtered dataframe\n",
    "    \"\"\"\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    # Date filtering\n",
    "    if start_date:\n",
    "        filtered_df = filtered_df[filtered_df['Toll Date'] >= pd.to_datetime(start_date)]\n",
    "    if end_date:\n",
    "        filtered_df = filtered_df[filtered_df['Toll Date'] <= pd.to_datetime(end_date)]\n",
    "    \n",
    "    # Day type filtering\n",
    "    if day_type:\n",
    "        if day_type.lower() == 'weekday':\n",
    "            # Monday(2) to Friday(6)\n",
    "            filtered_df = filtered_df[filtered_df['Day of Week Int'].between(2, 6)]\n",
    "        elif day_type.lower() == 'weekend':\n",
    "            # Saturday(7) and Sunday(1)\n",
    "            filtered_df = filtered_df[filtered_df['Day of Week Int'].isin([1, 7])]\n",
    "        else:\n",
    "            # Specific day\n",
    "            filtered_df = filtered_df[filtered_df['Day of Week'] == day_type]\n",
    "    \n",
    "    # Hour range filtering\n",
    "    if hour_range and len(hour_range) == 2:\n",
    "        start_hour, end_hour = hour_range\n",
    "        if start_hour <= end_hour:\n",
    "            filtered_df = filtered_df[filtered_df['Hour of Day'].between(start_hour, end_hour)]\n",
    "        else:\n",
    "            # Handle overnight ranges (e.g., 22-6)\n",
    "            filtered_df = filtered_df[(filtered_df['Hour of Day'] >= start_hour) | \n",
    "                                     (filtered_df['Hour of Day'] <= end_hour)]\n",
    "    \n",
    "    # Time period filtering\n",
    "    if time_period:\n",
    "        filtered_df = filtered_df[filtered_df['Time Period'] == time_period]\n",
    "    \n",
    "    # Vehicle class filtering\n",
    "    if vehicle_class:\n",
    "        # Handle both numeric and text representations\n",
    "        if isinstance(vehicle_class, int) or vehicle_class.isdigit():\n",
    "            class_num = int(vehicle_class)\n",
    "            filtered_df = filtered_df[filtered_df['Vehicle Class'].str.startswith(f\"{class_num} -\")]\n",
    "        else:\n",
    "            filtered_df = filtered_df[filtered_df['Vehicle Class'] == vehicle_class]\n",
    "    \n",
    "    # Entry point filtering\n",
    "    if entry_point:\n",
    "        filtered_df = filtered_df[filtered_df['Detection Group'] == entry_point]\n",
    "    \n",
    "    # Region filtering\n",
    "    if entry_region:\n",
    "        filtered_df = filtered_df[filtered_df['Detection Region'] == entry_region]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def analyze_entry_point_volume(df, \n",
    "                              top_n=10, \n",
    "                              start_date=None, \n",
    "                              end_date=None,\n",
    "                              day_type=None, \n",
    "                              hour_range=None,\n",
    "                              time_period=None,\n",
    "                              vehicle_class=None,\n",
    "                              include_excluded_roadways=False):\n",
    "    \"\"\"\n",
    "    Analyze traffic volumes for different entry points with customizable filters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The MTA CRZ dataset\n",
    "    top_n : int, optional\n",
    "        Number of top entry points to return\n",
    "    [filtering parameters as in filter_crz_data]\n",
    "    include_excluded_roadways : bool, optional\n",
    "        Whether to include entries on excluded roadways in the analysis\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Entry point analysis results including:\n",
    "        - top_entry_points: List of top entry points with volume and percentage\n",
    "        - total_volume: Total entry volume in the filtered dataset\n",
    "        - filter_summary: Summary of applied filters\n",
    "    \"\"\"\n",
    "    # Apply filters\n",
    "    filtered_df = filter_crz_data(df, \n",
    "                                 start_date=start_date, \n",
    "                                 end_date=end_date,\n",
    "                                 day_type=day_type, \n",
    "                                 hour_range=hour_range,\n",
    "                                 time_period=time_period,\n",
    "                                 vehicle_class=vehicle_class)\n",
    "    \n",
    "    # Group by entry point\n",
    "    if include_excluded_roadways:\n",
    "        # Sum both CRZ and Excluded Roadway entries\n",
    "        entry_volumes = filtered_df.groupby('Detection Group').agg({\n",
    "            'CRZ Entries': 'sum',\n",
    "            'Excluded Roadway Entries': 'sum'\n",
    "        })\n",
    "        entry_volumes['Total Entries'] = entry_volumes['CRZ Entries'] + entry_volumes['Excluded Roadway Entries']\n",
    "        volume_col = 'Total Entries'\n",
    "    else:\n",
    "        # Only count CRZ entries\n",
    "        entry_volumes = filtered_df.groupby('Detection Group').agg({\n",
    "            'CRZ Entries': 'sum'\n",
    "        })\n",
    "        volume_col = 'CRZ Entries'\n",
    "    \n",
    "    # Sort and get top N\n",
    "    top_entries = entry_volumes.sort_values(volume_col, ascending=False).head(top_n)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total_volume = entry_volumes[volume_col].sum()\n",
    "    top_entries['Percentage'] = (top_entries[volume_col] / total_volume * 100).round(1)\n",
    "    \n",
    "    # Create region mapping for context\n",
    "    region_mapping = filtered_df.drop_duplicates('Detection Group').set_index('Detection Group')['Detection Region']\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'top_entry_points': [\n",
    "            {\n",
    "                'entry_point': entry,\n",
    "                'region': region_mapping.get(entry, 'Unknown'),\n",
    "                'volume': int(row[volume_col]),\n",
    "                'percentage': float(row['Percentage'])\n",
    "            }\n",
    "            for entry, row in top_entries.iterrows()\n",
    "        ],\n",
    "        'total_volume': int(total_volume),\n",
    "        'filter_summary': {\n",
    "            'date_range': f\"{filtered_df['Toll Date'].min().strftime('%Y-%m-%d')} to {filtered_df['Toll Date'].max().strftime('%Y-%m-%d')}\" if not filtered_df.empty else \"No data\",\n",
    "            'day_type': day_type if day_type else \"All days\",\n",
    "            'hour_range': f\"{hour_range[0]}:00 to {hour_range[1]}:00\" if hour_range else \"All hours\",\n",
    "            'time_period': time_period if time_period else \"All periods\",\n",
    "            'vehicle_class': vehicle_class if vehicle_class else \"All vehicles\",\n",
    "            'entry_count': len(filtered_df['Detection Group'].unique())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_peak_periods(df, \n",
    "                        granularity='hour', \n",
    "                        top_n=5,\n",
    "                        start_date=None, \n",
    "                        end_date=None,\n",
    "                        day_type=None, \n",
    "                        vehicle_class=None,\n",
    "                        entry_point=None,\n",
    "                        entry_region=None):\n",
    "    \"\"\"\n",
    "    Identify peak traffic periods at different time granularities\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The MTA CRZ dataset\n",
    "    granularity : str, optional\n",
    "        Time granularity: 'hour', 'day_of_week', 'date', '10_minute'\n",
    "    top_n : int, optional\n",
    "        Number of peak periods to return\n",
    "    [filtering parameters as in filter_crz_data]\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Peak period analysis including:\n",
    "        - peak_periods: List of peak periods with volume\n",
    "        - peak_to_average_ratio: Ratio of peak volume to average volume\n",
    "        - total_volume: Total entry volume in the filtered dataset\n",
    "        - filter_summary: Summary of applied filters\n",
    "    \"\"\"\n",
    "    # Apply filters\n",
    "    filtered_df = filter_crz_data(df, \n",
    "                                 start_date=start_date, \n",
    "                                 end_date=end_date,\n",
    "                                 day_type=day_type,\n",
    "                                 vehicle_class=vehicle_class,\n",
    "                                 entry_point=entry_point,\n",
    "                                 entry_region=entry_region)\n",
    "    \n",
    "    # Group by chosen time granularity\n",
    "    if granularity == 'hour':\n",
    "        grouped = filtered_df.groupby('Hour of Day')['CRZ Entries'].sum().reset_index()\n",
    "        label_formatter = lambda x: f\"{int(x):02d}:00\"\n",
    "    \n",
    "    elif granularity == 'day_of_week':\n",
    "        # Order by actual day sequence (Monday to Sunday)\n",
    "        day_order = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, \n",
    "                    'Friday': 4, 'Saturday': 5, 'Sunday': 6}\n",
    "        grouped = filtered_df.groupby('Day of Week')['CRZ Entries'].sum().reset_index()\n",
    "        # Add ordering column and sort\n",
    "        grouped['day_order'] = grouped['Day of Week'].map(day_order)\n",
    "        grouped = grouped.sort_values('day_order')\n",
    "        grouped = grouped.drop('day_order', axis=1)\n",
    "        label_formatter = lambda x: x\n",
    "    \n",
    "    elif granularity == 'date':\n",
    "        grouped = filtered_df.groupby('Toll Date')['CRZ Entries'].sum().reset_index()\n",
    "        label_formatter = lambda x: x.strftime('%Y-%m-%d')\n",
    "    \n",
    "    elif granularity == '10_minute':\n",
    "        # Create a combined hour-minute column\n",
    "        filtered_df['time_block'] = filtered_df['Hour of Day'].astype(str) + ':' + filtered_df['Minute of Hour'].astype(str)\n",
    "        grouped = filtered_df.groupby('time_block')['CRZ Entries'].sum().reset_index()\n",
    "        label_formatter = lambda x: x\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported granularity: {granularity}\")\n",
    "    \n",
    "    # Sort by volume and get top peaks\n",
    "    grouped = grouped.sort_values('CRZ Entries', ascending=False)\n",
    "    top_periods = grouped.head(top_n)\n",
    "    \n",
    "    # Calculate average and peak-to-average ratio\n",
    "    average_volume = grouped['CRZ Entries'].mean()\n",
    "    peak_volume = grouped['CRZ Entries'].max()\n",
    "    peak_to_avg_ratio = peak_volume / average_volume if average_volume > 0 else 0\n",
    "    \n",
    "    # Prepare results\n",
    "    if granularity == 'hour':\n",
    "        x_label = 'hour'\n",
    "        x_value_col = 'Hour of Day'\n",
    "    elif granularity == 'day_of_week':\n",
    "        x_label = 'day'\n",
    "        x_value_col = 'Day of Week'\n",
    "    elif granularity == 'date':\n",
    "        x_label = 'date'\n",
    "        x_value_col = 'Toll Date'\n",
    "    else:\n",
    "        x_label = 'time_block'\n",
    "        x_value_col = 'time_block'\n",
    "    \n",
    "    results = {\n",
    "        'peak_periods': [\n",
    "            {\n",
    "                x_label: label_formatter(row[x_value_col]),\n",
    "                'volume': int(row['CRZ Entries']),\n",
    "                'percentage_of_total': float(row['CRZ Entries'] / grouped['CRZ Entries'].sum() * 100)\n",
    "            }\n",
    "            for _, row in top_periods.iterrows()\n",
    "        ],\n",
    "        'peak_to_average_ratio': float(peak_to_avg_ratio),\n",
    "        'average_volume': float(average_volume),\n",
    "        'total_volume': int(grouped['CRZ Entries'].sum()),\n",
    "        'filter_summary': {\n",
    "            'granularity': granularity,\n",
    "            'date_range': f\"{filtered_df['Toll Date'].min().strftime('%Y-%m-%d')} to {filtered_df['Toll Date'].max().strftime('%Y-%m-%d')}\" if not filtered_df.empty else \"No data\",\n",
    "            'day_type': day_type if day_type else \"All days\",\n",
    "            'vehicle_class': vehicle_class if vehicle_class else \"All vehicles\",\n",
    "            'entry_point': entry_point if entry_point else \"All entry points\",\n",
    "            'entry_region': entry_region if entry_region else \"All regions\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_vehicle_distribution(df,\n",
    "                               start_date=None, \n",
    "                               end_date=None,\n",
    "                               day_type=None, \n",
    "                               hour_range=None,\n",
    "                               time_period=None,\n",
    "                               entry_point=None,\n",
    "                               entry_region=None,\n",
    "                               compare_with=None):  # For comparison with another time period\n",
    "    \"\"\"\n",
    "    Analyze distribution of traffic by vehicle type with optional comparison\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The MTA CRZ dataset\n",
    "    [filtering parameters as in filter_crz_data]\n",
    "    compare_with : dict, optional\n",
    "        Dictionary with filter parameters for comparison period\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Vehicle distribution analysis including:\n",
    "        - vehicle_distribution: Breakdown by vehicle type\n",
    "        - total_volume: Total entry volume\n",
    "        - comparison: Optional comparison with another time period\n",
    "        - filter_summary: Summary of applied filters\n",
    "    \"\"\"\n",
    "    # Apply filters for main period\n",
    "    filtered_df = filter_crz_data(df, \n",
    "                                 start_date=start_date, \n",
    "                                 end_date=end_date,\n",
    "                                 day_type=day_type, \n",
    "                                 hour_range=hour_range,\n",
    "                                 time_period=time_period,\n",
    "                                 entry_point=entry_point,\n",
    "                                 entry_region=entry_region)\n",
    "    \n",
    "    # Group by vehicle class\n",
    "    vehicle_counts = filtered_df.groupby('Vehicle Class')['CRZ Entries'].sum().reset_index()\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total_volume = vehicle_counts['CRZ Entries'].sum()\n",
    "    vehicle_counts['Percentage'] = (vehicle_counts['CRZ Entries'] / total_volume * 100).round(1)\n",
    "    \n",
    "    # Sort by volume\n",
    "    vehicle_counts = vehicle_counts.sort_values('CRZ Entries', ascending=False)\n",
    "    \n",
    "    # Prepare comparison if requested\n",
    "    comparison_data = None\n",
    "    if compare_with:\n",
    "        # Apply filters for comparison period\n",
    "        comp_df = filter_crz_data(df, **compare_with)\n",
    "        \n",
    "        # Group by vehicle class\n",
    "        comp_counts = comp_df.groupby('Vehicle Class')['CRZ Entries'].sum().reset_index()\n",
    "        \n",
    "        # Calculate percentages\n",
    "        comp_total = comp_counts['CRZ Entries'].sum()\n",
    "        comp_counts['Percentage'] = (comp_counts['CRZ Entries'] / comp_total * 100).round(1)\n",
    "        \n",
    "        # Create comparison dataset\n",
    "        comparison_data = {\n",
    "            'vehicle_distribution': [\n",
    "                {\n",
    "                    'vehicle_class': row['Vehicle Class'],\n",
    "                    'volume': int(row['CRZ Entries']),\n",
    "                    'percentage': float(row['Percentage'])\n",
    "                }\n",
    "                for _, row in comp_counts.sort_values('CRZ Entries', ascending=False).iterrows()\n",
    "            ],\n",
    "            'total_volume': int(comp_total),\n",
    "            'filter_summary': {key: value for key, value in compare_with.items() if value is not None}\n",
    "        }\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'vehicle_distribution': [\n",
    "            {\n",
    "                'vehicle_class': row['Vehicle Class'],\n",
    "                'volume': int(row['CRZ Entries']),\n",
    "                'percentage': float(row['Percentage'])\n",
    "            }\n",
    "            for _, row in vehicle_counts.iterrows()\n",
    "        ],\n",
    "        'total_volume': int(total_volume),\n",
    "        'filter_summary': {\n",
    "            'date_range': f\"{filtered_df['Toll Date'].min().strftime('%Y-%m-%d')} to {filtered_df['Toll Date'].max().strftime('%Y-%m-%d')}\" if not filtered_df.empty else \"No data\",\n",
    "            'day_type': day_type if day_type else \"All days\",\n",
    "            'hour_range': f\"{hour_range[0]}:00 to {hour_range[1]}:00\" if hour_range else \"All hours\",\n",
    "            'time_period': time_period if time_period else \"All periods\",\n",
    "            'entry_point': entry_point if entry_point else \"All entry points\",\n",
    "            'entry_region': entry_region if entry_region else \"All regions\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if comparison_data:\n",
    "        results['comparison'] = comparison_data\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_time_trends(df, \n",
    "                      metric='CRZ Entries',\n",
    "                      time_unit='day',  # 'hour', 'day', 'week', 'month'\n",
    "                      start_date=None, \n",
    "                      end_date=None,\n",
    "                      day_type=None,\n",
    "                      vehicle_class=None,\n",
    "                      entry_point=None,\n",
    "                      entry_region=None):\n",
    "    \"\"\"\n",
    "    Analyze traffic trends over time with different aggregation levels\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The MTA CRZ dataset\n",
    "    metric : str, optional\n",
    "        Metric to analyze: 'CRZ Entries' or 'Excluded Roadway Entries'\n",
    "    time_unit : str, optional\n",
    "        Time unit for aggregation: 'hour', 'day', 'week', 'month'\n",
    "    [filtering parameters as in filter_crz_data]\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Time trend analysis including:\n",
    "        - time_series: List of time points with volumes\n",
    "        - trend_stats: Statistics about the trend (growth rate, etc.)\n",
    "        - filter_summary: Summary of applied filters\n",
    "    \"\"\"\n",
    "    # Apply filters\n",
    "    filtered_df = filter_crz_data(df, \n",
    "                                 start_date=start_date, \n",
    "                                 end_date=end_date,\n",
    "                                 day_type=day_type,\n",
    "                                 vehicle_class=vehicle_class,\n",
    "                                 entry_point=entry_point,\n",
    "                                 entry_region=entry_region)\n",
    "    \n",
    "    # Group by time unit\n",
    "    if time_unit == 'hour':\n",
    "        grouped = filtered_df.groupby('Hour of Day')[metric].sum().reset_index()\n",
    "        x_label = 'hour'\n",
    "        x_column = 'Hour of Day'\n",
    "        formatter = lambda x: f\"{int(x):02d}:00\"\n",
    "        \n",
    "    elif time_unit == 'day':\n",
    "        grouped = filtered_df.groupby('Toll Date')[metric].sum().reset_index()\n",
    "        x_label = 'date'\n",
    "        x_column = 'Toll Date'\n",
    "        formatter = lambda x: x.strftime('%Y-%m-%d')\n",
    "        \n",
    "    elif time_unit == 'day_of_week':\n",
    "        # Map days to numbers for proper ordering\n",
    "        day_order = {'Sunday': 0, 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, \n",
    "                    'Thursday': 4, 'Friday': 5, 'Saturday': 6}\n",
    "        grouped = filtered_df.groupby('Day of Week')[metric].sum().reset_index()\n",
    "        grouped['day_order'] = grouped['Day of Week'].map(day_order)\n",
    "        grouped = grouped.sort_values('day_order')\n",
    "        grouped = grouped.drop('day_order', axis=1)\n",
    "        x_label = 'day'\n",
    "        x_column = 'Day of Week'\n",
    "        formatter = lambda x: x\n",
    "        \n",
    "    elif time_unit == 'week':\n",
    "        grouped = filtered_df.groupby('Toll Week')[metric].sum().reset_index()\n",
    "        x_label = 'week'\n",
    "        x_column = 'Toll Week'\n",
    "        formatter = lambda x: x.strftime('%Y-%m-%d')\n",
    "        \n",
    "    elif time_unit == 'month':\n",
    "        # Create a month column if it doesn't exist\n",
    "        if 'Month' not in filtered_df.columns and 'Toll Date' in filtered_df.columns:\n",
    "            filtered_df['Month'] = filtered_df['Toll Date'].dt.to_period('M')\n",
    "        grouped = filtered_df.groupby('Month')[metric].sum().reset_index()\n",
    "        x_label = 'month'\n",
    "        x_column = 'Month'\n",
    "        formatter = lambda x: str(x)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported time unit: {time_unit}\")\n",
    "    \n",
    "    # Calculate trend statistics\n",
    "    if len(grouped) > 1 and time_unit in ['day', 'week', 'month']:\n",
    "        # Sort chronologically for trend calculation\n",
    "        grouped = grouped.sort_values(x_column)\n",
    "        \n",
    "        # Calculate growth metrics\n",
    "        first_value = grouped[metric].iloc[0]\n",
    "        last_value = grouped[metric].iloc[-1]\n",
    "        total_growth = last_value - first_value\n",
    "        percent_growth = (total_growth / first_value * 100) if first_value > 0 else 0\n",
    "        \n",
    "        # Calculate average daily change\n",
    "        if time_unit == 'day':\n",
    "            daily_changes = grouped[metric].diff().dropna()\n",
    "            avg_daily_change = daily_changes.mean()\n",
    "        else:\n",
    "            avg_daily_change = None\n",
    "            \n",
    "        trend_stats = {\n",
    "            'total_growth': float(total_growth),\n",
    "            'percent_growth': float(percent_growth),\n",
    "            'avg_daily_change': float(avg_daily_change) if avg_daily_change is not None else None,\n",
    "            'min_value': float(grouped[metric].min()),\n",
    "            'max_value': float(grouped[metric].max()),\n",
    "            'std_dev': float(grouped[metric].std())\n",
    "        }\n",
    "    else:\n",
    "        trend_stats = {\n",
    "            'min_value': float(grouped[metric].min()) if not grouped.empty else 0,\n",
    "            'max_value': float(grouped[metric].max()) if not grouped.empty else 0,\n",
    "            'avg_value': float(grouped[metric].mean()) if not grouped.empty else 0\n",
    "        }\n",
    "    \n",
    "    # Prepare time series data\n",
    "    time_series = [\n",
    "        {\n",
    "            x_label: formatter(row[x_column]),\n",
    "            'volume': int(row[metric])\n",
    "        }\n",
    "        for _, row in grouped.iterrows()\n",
    "    ]\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'time_series': time_series,\n",
    "        'trend_stats': trend_stats,\n",
    "        'metric': metric,\n",
    "        'time_unit': time_unit,\n",
    "        'total_volume': int(grouped[metric].sum()),\n",
    "        'filter_summary': {\n",
    "            'date_range': f\"{filtered_df['Toll Date'].min().strftime('%Y-%m-%d')} to {filtered_df['Toll Date'].max().strftime('%Y-%m-%d')}\" if not filtered_df.empty else \"No data\",\n",
    "            'day_type': day_type if day_type else \"All days\",\n",
    "            'vehicle_class': vehicle_class if vehicle_class else \"All vehicles\",\n",
    "            'entry_point': entry_point if entry_point else \"All entry points\",\n",
    "            'entry_region': entry_region if entry_region else \"All regions\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_excluded_roadway_usage(df,\n",
    "                                 start_date=None, \n",
    "                                 end_date=None,\n",
    "                                 day_type=None, \n",
    "                                 hour_range=None,\n",
    "                                 time_period=None,\n",
    "                                 vehicle_class=None,\n",
    "                                 entry_region=None):\n",
    "    \"\"\"\n",
    "    Analyze usage patterns of excluded roadways vs. congestion zone\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The MTA CRZ dataset\n",
    "    [filtering parameters as in filter_crz_data]\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Excluded roadway usage analysis including:\n",
    "        - overall_usage: Breakdown of CRZ vs excluded roadway usage\n",
    "        - by_entry_point: Usage breakdown by entry point\n",
    "        - by_vehicle_class: Usage breakdown by vehicle class\n",
    "        - by_time: Usage patterns by time\n",
    "        - filter_summary: Summary of applied filters\n",
    "    \"\"\"\n",
    "    # Apply filters\n",
    "    filtered_df = filter_crz_data(df, \n",
    "                                 start_date=start_date, \n",
    "                                 end_date=end_date,\n",
    "                                 day_type=day_type, \n",
    "                                 hour_range=hour_range,\n",
    "                                 time_period=time_period,\n",
    "                                 vehicle_class=vehicle_class,\n",
    "                                 entry_region=entry_region)\n",
    "    \n",
    "    # Calculate overall usage\n",
    "    total_crz = filtered_df['CRZ Entries'].sum()\n",
    "    total_excluded = filtered_df['Excluded Roadway Entries'].sum()\n",
    "    total_entries = total_crz + total_excluded\n",
    "    \n",
    "    # Usage by entry point\n",
    "    entry_usage = filtered_df.groupby('Detection Group').agg({\n",
    "        'CRZ Entries': 'sum',\n",
    "        'Excluded Roadway Entries': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate total and excluded percentage for each entry point\n",
    "    entry_usage['Total'] = entry_usage['CRZ Entries'] + entry_usage['Excluded Roadway Entries']\n",
    "    entry_usage['Excluded Percentage'] = (entry_usage['Excluded Roadway Entries'] / \n",
    "                                        entry_usage['Total'] * 100).round(1)\n",
    "    \n",
    "    # Sort by excluded percentage\n",
    "    entry_usage = entry_usage.sort_values('Excluded Percentage', ascending=False)\n",
    "    \n",
    "    # Usage by vehicle class\n",
    "    vehicle_usage = filtered_df.groupby('Vehicle Class').agg({\n",
    "        'CRZ Entries': 'sum',\n",
    "        'Excluded Roadway Entries': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate total and excluded percentage for each vehicle class\n",
    "    vehicle_usage['Total'] = vehicle_usage['CRZ Entries'] + vehicle_usage['Excluded Roadway Entries']\n",
    "    vehicle_usage['Excluded Percentage'] = (vehicle_usage['Excluded Roadway Entries'] / \n",
    "                                         vehicle_usage['Total'] * 100).round(1)\n",
    "    \n",
    "    # Usage by time\n",
    "    # Group by hour of day\n",
    "    hourly_usage = filtered_df.groupby('Hour of Day').agg({\n",
    "        'CRZ Entries': 'sum',\n",
    "        'Excluded Roadway Entries': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    hourly_usage['Total'] = hourly_usage['CRZ Entries'] + hourly_usage['Excluded Roadway Entries']\n",
    "    hourly_usage['Excluded Percentage'] = (hourly_usage['Excluded Roadway Entries'] / \n",
    "                                         hourly_usage['Total'] * 100).round(1)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'overall_usage': {\n",
    "            'total_entries': int(total_entries),\n",
    "            'crz_entries': int(total_crz),\n",
    "            'excluded_entries': int(total_excluded),\n",
    "            'excluded_percentage': float(total_excluded / total_entries * 100) if total_entries > 0 else 0\n",
    "        },\n",
    "        'by_entry_point': [\n",
    "            {\n",
    "                'entry_point': row['Detection Group'],\n",
    "                'crz_entries': int(row['CRZ Entries']),\n",
    "                'excluded_entries': int(row['Excluded Roadway Entries']),\n",
    "                'total': int(row['Total']),\n",
    "                'excluded_percentage': float(row['Excluded Percentage'])\n",
    "            }\n",
    "            for _, row in entry_usage.head(10).iterrows()  # Top 10 for brevity\n",
    "        ],\n",
    "        'by_vehicle_class': [\n",
    "            {\n",
    "                'vehicle_class': row['Vehicle Class'],\n",
    "                'crz_entries': int(row['CRZ Entries']),\n",
    "                'excluded_entries': int(row['Excluded Roadway Entries']),\n",
    "                'total': int(row['Total']),\n",
    "                'excluded_percentage': float(row['Excluded Percentage'])\n",
    "            }\n",
    "            for _, row in vehicle_usage.iterrows()\n",
    "        ],\n",
    "        'by_time': [\n",
    "            {\n",
    "                'hour': int(row['Hour of Day']),\n",
    "                'crz_entries': int(row['CRZ Entries']),\n",
    "                'excluded_entries': int(row['Excluded Roadway Entries']),\n",
    "                'excluded_percentage': float(row['Excluded Percentage'])\n",
    "            }\n",
    "            for _, row in hourly_usage.iterrows()\n",
    "        ],\n",
    "        'filter_summary': {\n",
    "            'date_range': f\"{filtered_df['Toll Date'].min().strftime('%Y-%m-%d')} to {filtered_df['Toll Date'].max().strftime('%Y-%m-%d')}\" if not filtered_df.empty else \"No data\",\n",
    "            'day_type': day_type if day_type else \"All days\",\n",
    "            'hour_range': f\"{hour_range[0]}:00 to {hour_range[1]}:00\" if hour_range else \"All hours\",\n",
    "            'time_period': time_period if time_period else \"All periods\",\n",
    "            'vehicle_class': vehicle_class if vehicle_class else \"All vehicles\",\n",
    "            'entry_region': entry_region if entry_region else \"All regions\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_traffic_segments(df,\n",
    "                           dimension='time',  # 'time', 'vehicle', 'location'\n",
    "                           segment_a=None,\n",
    "                           segment_b=None,\n",
    "                           metric='CRZ Entries'):\n",
    "    \"\"\"\n",
    "    Compare traffic patterns between two segments (time periods, vehicle types, or locations)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The MTA CRZ dataset\n",
    "    dimension : str\n",
    "        Dimension to compare: 'time', 'vehicle', or 'location'\n",
    "    segment_a : dict\n",
    "        Filter parameters for first segment\n",
    "    segment_b : dict\n",
    "        Filter parameters for second segment\n",
    "    metric : str, optional\n",
    "        Metric to compare: 'CRZ Entries' or 'Excluded Roadway Entries'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Comparison results including:\n",
    "        - segment_a_stats: Statistics for first segment\n",
    "        - segment_b_stats: Statistics for second segment\n",
    "        - differences: Key differences between segments\n",
    "        - common_patterns: Common patterns between segments\n",
    "    \"\"\"\n",
    "    # Validate segments\n",
    "    if not segment_a or not segment_b:\n",
    "        raise ValueError(\"Both segment_a and segment_b must be provided\")\n",
    "    \n",
    "    # Apply filters for segment A\n",
    "    df_a = filter_crz_data(df, **segment_a)\n",
    "    \n",
    "    # Apply filters for segment B\n",
    "    df_b = filter_crz_data(df, **segment_b)\n",
    "    \n",
    "    # Analysis varies by dimension\n",
    "    if dimension == 'time':\n",
    "        # For time comparison, we look at patterns across other dimensions\n",
    "        \n",
    "        # Vehicle class distribution\n",
    "        vehicle_a = df_a.groupby('Vehicle Class')[metric].sum()\n",
    "        total_a = vehicle_a.sum()\n",
    "        vehicle_a_pct = (vehicle_a / total_a * 100).round(1) if total_a > 0 else vehicle_a * 0\n",
    "        \n",
    "        vehicle_b = df_b.groupby('Vehicle Class')[metric].sum()\n",
    "        total_b = vehicle_b.sum()\n",
    "        vehicle_b_pct = (vehicle_b / total_b * 100).round(1) if total_b > 0 else vehicle_b * 0\n",
    "        \n",
    "        # Entry point distribution\n",
    "        entry_a = df_a.groupby('Detection Group')[metric].sum()\n",
    "        entry_a_pct = (entry_a / total_a * 100).round(1) if total_a > 0 else entry_a * 0\n",
    "        \n",
    "        entry_b = df_b.groupby('Detection Group')[metric].sum()\n",
    "        entry_b_pct = (entry_b / total_b * 100).round(1) if total_b > 0 else entry_b * 0\n",
    "        \n",
    "        # Calculate differences in percentages for vehicle classes\n",
    "        vehicle_diff = {}\n",
    "        for vehicle in set(vehicle_a.index) | set(vehicle_b.index):\n",
    "            pct_a = vehicle_a_pct.get(vehicle, 0)\n",
    "            pct_b = vehicle_b_pct.get(vehicle, 0)\n",
    "            vehicle_diff[vehicle] = float(pct_b - pct_a)\n",
    "        \n",
    "        # Find top entry point differences\n",
    "        entry_diff = {}\n",
    "        for entry in set(entry_a.index) | set(entry_b.index):\n",
    "            pct_a = entry_a_pct.get(entry, 0)\n",
    "            pct_b = entry_b_pct.get(entry, 0)\n",
    "            entry_diff[entry] = float(pct_b - pct_a)\n",
    "        \n",
    "        # Sort differences\n",
    "        vehicle_diff = dict(sorted(vehicle_diff.items(), key=lambda x: abs(x[1]), reverse=True))\n",
    "        entry_diff = dict(sorted(entry_diff.items(), key=lambda x: abs(x[1]), reverse=True))\n",
    "        \n",
    "        # Prepare results\n",
    "        segment_a_name = f\"Period A: {segment_a.get('day_type', 'All days')}\"\n",
    "        if 'hour_range' in segment_a and segment_a['hour_range']:\n",
    "            segment_a_name += f\", {segment_a['hour_range'][0]}-{segment_a['hour_range'][1]} hours\"\n",
    "        \n",
    "        segment_b_name = f\"Period B: {segment_b.get('day_type', 'All days')}\"\n",
    "        if 'hour_range' in segment_b and segment_b['hour_range']:\n",
    "            segment_b_name += f\", {segment_b['hour_range'][0]}-{segment_b['hour_range'][1]} hours\"\n",
    "        \n",
    "        results = {\n",
    "            'comparison_type': 'Time periods',\n",
    "            'segment_a': {\n",
    "                'name': segment_a_name,\n",
    "                'total_volume': int(total_a),\n",
    "                'vehicle_distribution': vehicle_a_pct.to_dict(),\n",
    "                'top_entry_points': entry_a_pct.nlargest(5).to_dict()\n",
    "            },\n",
    "            'segment_b': {\n",
    "                'name': segment_b_name,\n",
    "                'total_volume': int(total_b),\n",
    "                'vehicle_distribution': vehicle_b_pct.to_dict(),\n",
    "                'top_entry_points': entry_b_pct.nlargest(5).to_dict()\n",
    "            },\n",
    "            'differences': {\n",
    "                'total_volume_diff': int(total_b - total_a),\n",
    "                'total_volume_pct_diff': float((total_b - total_a) / total_a * 100) if total_a > 0 else 0,\n",
    "                'vehicle_distribution_diff': {k: v for k, v in list(vehicle_diff.items())[:5]},  # Top 5\n",
    "                'entry_point_diff': {k: v for k, v in list(entry_diff.items())[:5]}  # Top 5\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    elif dimension == 'vehicle':\n",
    "        # For vehicle comparison, we look at time and location patterns\n",
    "        \n",
    "        # Time patterns - hour of day\n",
    "        hour_a = df_a.groupby('Hour of Day')[metric].sum()\n",
    "        total_a = hour_a.sum()\n",
    "        hour_a_pct = (hour_a / total_a * 100).round(1) if total_a > 0 else hour_a * 0\n",
    "        \n",
    "        hour_b = df_b.groupby('Hour of Day')[metric].sum()\n",
    "        total_b = hour_b.sum()\n",
    "        hour_b_pct = (hour_b / total_b * 100).round(1) if total_b > 0 else hour_b * 0\n",
    "        \n",
    "        # Day of week patterns\n",
    "        day_a = df_a.groupby('Day of Week')[metric].sum()\n",
    "        day_a_pct = (day_a / total_a * 100).round(1) if total_a > 0 else day_a * 0\n",
    "        \n",
    "        day_b = df_b.groupby('Day of Week')[metric].sum()\n",
    "        day_b_pct = (day_b / total_b * 100).round(1) if total_b > 0 else day_b * 0\n",
    "        \n",
    "        # Entry point patterns\n",
    "        entry_a = df_a.groupby('Detection Group')[metric].sum()\n",
    "        entry_a_pct = (entry_a / total_a * 100).round(1) if total_a > 0 else entry_a * 0\n",
    "        \n",
    "        entry_b = df_b.groupby('Detection Group')[metric].sum()\n",
    "        entry_b_pct = (entry_b / total_b * 100).round(1) if total_b > 0 else entry_b * 0\n",
    "        \n",
    "        # Calculate differences\n",
    "        hour_diff = {}\n",
    "        for hour in range(24):\n",
    "            pct_a = hour_a_pct.get(hour, 0)\n",
    "            pct_b = hour_b_pct.get(hour, 0)\n",
    "            hour_diff[hour] = float(pct_b - pct_a)\n",
    "        \n",
    "        # Find peak hours\n",
    "        peak_hour_a = hour_a.idxmax() if not hour_a.empty else None\n",
    "        peak_hour_b = hour_b.idxmax() if not hour_b.empty else None\n",
    "        \n",
    "        # Sort differences\n",
    "        hour_diff = dict(sorted(hour_diff.items(), key=lambda x: abs(x[1]), reverse=True))\n",
    "        \n",
    "        # Prepare results\n",
    "        segment_a_name = f\"Vehicle A: {segment_a.get('vehicle_class', 'All vehicles')}\"\n",
    "        segment_b_name = f\"Vehicle B: {segment_b.get('vehicle_class', 'All vehicles')}\"\n",
    "        \n",
    "        results = {\n",
    "            'comparison_type': 'Vehicle classes',\n",
    "            'segment_a': {\n",
    "                'name': segment_a_name,\n",
    "                'total_volume': int(total_a),\n",
    "                'peak_hour': int(peak_hour_a) if peak_hour_a is not None else None,\n",
    "                'hourly_distribution': hour_a_pct.to_dict(),\n",
    "                'day_distribution': day_a_pct.to_dict(),\n",
    "                'top_entry_points': entry_a_pct.nlargest(5).to_dict()\n",
    "            },\n",
    "            'segment_b': {\n",
    "                'name': segment_b_name,\n",
    "                'total_volume': int(total_b),\n",
    "                'peak_hour': int(peak_hour_b) if peak_hour_b is not None else None,\n",
    "                'hourly_distribution': hour_b_pct.to_dict(),\n",
    "                'day_distribution': day_b_pct.to_dict(),\n",
    "                'top_entry_points': entry_b_pct.nlargest(5).to_dict()\n",
    "            },\n",
    "            'differences': {\n",
    "                'total_volume_diff': int(total_b - total_a),\n",
    "                'total_volume_pct_diff': float((total_b - total_a) / total_a * 100) if total_a > 0 else 0,\n",
    "                'hour_distribution_diff': {str(k): v for k, v in list(hour_diff.items())[:5]},  # Top 5\n",
    "                'peak_hour_diff': int(peak_hour_b - peak_hour_a) if peak_hour_a is not None and peak_hour_b is not None else None\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    elif dimension == 'location':\n",
    "        # For location comparison, we look at time and vehicle patterns\n",
    "        \n",
    "        # Time patterns - hour of day\n",
    "        hour_a = df_a.groupby('Hour of Day')[metric].sum()\n",
    "        total_a = hour_a.sum()\n",
    "        hour_a_pct = (hour_a / total_a * 100).round(1) if total_a > 0 else hour_a * 0\n",
    "        \n",
    "        hour_b = df_b.groupby('Hour of Day')[metric].sum()\n",
    "        total_b = hour_b.sum()\n",
    "        hour_b_pct = (hour_b / total_b * 100).round(1) if total_b > 0 else hour_b * 0\n",
    "        \n",
    "        # Vehicle patterns\n",
    "        vehicle_a = df_a.groupby('Vehicle Class')[metric].sum()\n",
    "        vehicle_a_pct = (vehicle_a / total_a * 100).round(1) if total_a > 0 else vehicle_a * 0\n",
    "        \n",
    "        vehicle_b = df_b.groupby('Vehicle Class')[metric].sum()\n",
    "        vehicle_b_pct = (vehicle_b / total_b * 100).round(1) if total_b > 0 else vehicle_b * 0\n",
    "        \n",
    "        # Calculate differences\n",
    "        vehicle_diff = {}\n",
    "        for vehicle in set(vehicle_a.index) | set(vehicle_b.index):\n",
    "            pct_a = vehicle_a_pct.get(vehicle, 0)\n",
    "            pct_b = vehicle_b_pct.get(vehicle, 0)\n",
    "            vehicle_diff[vehicle] = float(pct_b - pct_a)\n",
    "        \n",
    "        # Find peak hours\n",
    "        peak_hour_a = hour_a.idxmax() if not hour_a.empty else None\n",
    "        peak_hour_b = hour_b.idxmax() if not hour_b.empty else None\n",
    "        \n",
    "        # Excluded roadway usage\n",
    "        excluded_a = df_a['Excluded Roadway Entries'].sum()\n",
    "        excluded_pct_a = (excluded_a / (total_a + excluded_a) * 100).round(1) if (total_a + excluded_a) > 0 else 0\n",
    "        \n",
    "        excluded_b = df_b['Excluded Roadway Entries'].sum()\n",
    "        excluded_pct_b = (excluded_b / (total_b + excluded_b) * 100).round(1) if (total_b + excluded_b) > 0 else 0\n",
    "        \n",
    "        # Prepare results\n",
    "        segment_a_name = f\"Location A: {segment_a.get('entry_point', segment_a.get('entry_region', 'All locations'))}\"\n",
    "        segment_b_name = f\"Location B: {segment_b.get('entry_point', segment_b.get('entry_region', 'All locations'))}\"\n",
    "        \n",
    "        results = {\n",
    "            'comparison_type': 'Locations',\n",
    "            'segment_a': {\n",
    "                'name': segment_a_name,\n",
    "                'total_volume': int(total_a),\n",
    "                'peak_hour': int(peak_hour_a) if peak_hour_a is not None else None,\n",
    "                'hourly_distribution': hour_a_pct.to_dict(),\n",
    "                'vehicle_distribution': vehicle_a_pct.to_dict(),\n",
    "                'excluded_roadway_percentage': float(excluded_pct_a)\n",
    "            },\n",
    "            'segment_b': {\n",
    "                'name': segment_b_name,\n",
    "                'total_volume': int(total_b),\n",
    "                'peak_hour': int(peak_hour_b) if peak_hour_b is not None else None,\n",
    "                'hourly_distribution': hour_b_pct.to_dict(),\n",
    "                'vehicle_distribution': vehicle_b_pct.to_dict(),\n",
    "                'excluded_roadway_percentage': float(excluded_pct_b)\n",
    "            },\n",
    "            'differences': {\n",
    "                'total_volume_diff': int(total_b - total_a),\n",
    "                'total_volume_pct_diff': float((total_b - total_a) / total_a * 100) if total_a > 0 else 0,\n",
    "                'vehicle_distribution_diff': {k: v for k, v in list(vehicle_diff.items())[:5]},  # Top 5\n",
    "                'peak_hour_diff': int(peak_hour_b - peak_hour_a) if peak_hour_a is not None and peak_hour_b is not None else None,\n",
    "                'excluded_roadway_pct_diff': float(excluded_pct_b - excluded_pct_a)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported comparison dimension: {dimension}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = {\n",
    "  \"functions\": [\n",
    "    {\n",
    "      \"function_name\": \"filter_crz_data\",\n",
    "      \"description\": \"Filter the CRZ dataset by multiple parameters\",\n",
    "      \"required_parameters\": [],\n",
    "      \"optional_parameters\": [\n",
    "        \"start_date\", \"end_date\", \"day_type\", \"hour_range\", \"time_period\", \n",
    "        \"vehicle_class\", \"entry_point\", \"entry_region\"\n",
    "      ],\n",
    "      \"returns\": \"Filtered dataframe for further analysis\"\n",
    "    },\n",
    "    {\n",
    "      \"function_name\": \"analyze_entry_point_volume\",\n",
    "      \"description\": \"Analyze traffic volumes for different entry points\",\n",
    "      \"required_parameters\": [],\n",
    "      \"optional_parameters\": [\n",
    "        \"start_date\", \"end_date\", \"day_type\", \"hour_range\", \"time_period\", \n",
    "        \"vehicle_class\", \"top_n\", \"include_excluded_roadways\"\n",
    "      ],\n",
    "      \"returns\": \"Dictionary with top entry points by volume and percentage\"\n",
    "    },\n",
    "    {\n",
    "      \"function_name\": \"analyze_peak_periods\",\n",
    "      \"description\": \"Identify peak traffic periods at different time granularities\",\n",
    "      \"required_parameters\": [],\n",
    "      \"optional_parameters\": [\n",
    "        \"start_date\", \"end_date\", \"day_type\", \"vehicle_class\", \"entry_point\", \n",
    "        \"entry_region\", \"granularity\", \"top_n\"\n",
    "      ],\n",
    "      \"returns\": \"Dictionary with peak periods and volumes\"\n",
    "    },\n",
    "    {\n",
    "      \"function_name\": \"analyze_vehicle_distribution\",\n",
    "      \"description\": \"Analyze distribution of traffic by vehicle type\",\n",
    "      \"required_parameters\": [],\n",
    "      \"optional_parameters\": [\n",
    "        \"start_date\", \"end_date\", \"day_type\", \"hour_range\", \"time_period\", \n",
    "        \"entry_point\", \"entry_region\", \"compare_with\"\n",
    "      ],\n",
    "      \"returns\": \"Dictionary with vehicle distribution breakdown\"\n",
    "    },\n",
    "    {\n",
    "      \"function_name\": \"analyze_time_trends\",\n",
    "      \"description\": \"Analyze traffic trends over time\",\n",
    "      \"required_parameters\": [],\n",
    "      \"optional_parameters\": [\n",
    "        \"start_date\", \"end_date\", \"day_type\", \"vehicle_class\", \"entry_point\", \n",
    "        \"entry_region\", \"metric\", \"time_unit\"\n",
    "      ],\n",
    "      \"returns\": \"Dictionary with time series data and trend statistics\"\n",
    "    },\n",
    "    {\n",
    "      \"function_name\": \"analyze_excluded_roadway_usage\",\n",
    "      \"description\": \"Analyze usage patterns of excluded roadways vs. congestion zone\",\n",
    "      \"required_parameters\": [],\n",
    "      \"optional_parameters\": [\n",
    "        \"start_date\", \"end_date\", \"day_type\", \"hour_range\", \"time_period\", \n",
    "        \"vehicle_class\", \"entry_region\"\n",
    "      ],\n",
    "      \"returns\": \"Dictionary with excluded roadway usage analysis\"\n",
    "    },\n",
    "    {\n",
    "      \"function_name\": \"analyze_vehicle_patterns\",\n",
    "      \"description\": \"Analyze traffic patterns for a specific vehicle class\",\n",
    "      \"required_parameters\": [\"vehicle_class\"],\n",
    "      \"optional_parameters\": [\n",
    "        \"start_date\", \"end_date\", \"day_type\", \"hour_range\", \"time_period\", \n",
    "        \"entry_point\", \"entry_region\"\n",
    "      ],\n",
    "      \"returns\": \"Dictionary with vehicle pattern analysis including time and location patterns\"\n",
    "    },\n",
    "    {\n",
    "      \"function_name\": \"compare_traffic_segments\",\n",
    "      \"description\": \"Compare traffic patterns between two segments\",\n",
    "      \"required_parameters\": [\"dimension\", \"segment_a\", \"segment_b\"],\n",
    "      \"optional_parameters\": [\"metric\"],\n",
    "      \"returns\": \"Dictionary with comparison results between segments\"\n",
    "    },\n",
    "    {\n",
    "      \"function_name\": \"generate_visualization\",\n",
    "      \"description\": \"Generate a visualization based on the data\",\n",
    "      \"required_parameters\": [\"chart_type\", \"x_column\", \"y_column\"],\n",
    "      \"optional_parameters\": [\n",
    "        \"title\", \"start_date\", \"end_date\", \"day_type\", \"hour_range\", \n",
    "        \"time_period\", \"vehicle_class\", \"entry_point\", \"entry_region\"\n",
    "      ],\n",
    "      \"returns\": \"Path to saved visualization or visualization code\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_function_description(function_name: str) -> str:\n",
    "    for function in functions['functions']:\n",
    "        if function['function_name'] == function_name:\n",
    "            return function\n",
    "    return \"Function not found\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('..'))))\n",
    "from src.llm.api_call import call_llm\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List, Tuple, Union, Dict, Any, Type, Literal\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any, Type\n",
    "\n",
    "class FunctionParams:\n",
    "    \"\"\"Container for all function parameter models\"\"\"\n",
    "    \n",
    "    class FilterCRZDataParams(BaseModel):\n",
    "        \"\"\"Parameters for filter_crz_data function\"\"\"\n",
    "        start_date: str | None = None\n",
    "        end_date: str | None = None\n",
    "        day_type: str | None = None\n",
    "        hour_range: List[int] | None = None  # Changed from Tuple to List\n",
    "        time_period: str | None = None\n",
    "        vehicle_class: str | None = None\n",
    "        entry_point: str | None = None\n",
    "        entry_region: str | None = None\n",
    "    \n",
    "    class AnalyzeEntryPointVolumeParams(BaseModel):\n",
    "        \"\"\"Parameters for analyze_entry_point_volume function\"\"\"\n",
    "        start_date: str | None = None\n",
    "        end_date: str | None = None\n",
    "        day_type: str | None = None\n",
    "        hour_range: List[int] | None = None  # Changed from Tuple to List\n",
    "        time_period: str | None = None\n",
    "        vehicle_class: str | None = None\n",
    "        entry_point: str | None = None\n",
    "        entry_region: str | None = None\n",
    "        top_n: int | None = 10\n",
    "        include_excluded_roadways: bool | None = False\n",
    "    \n",
    "    class AnalyzePeakPeriodsParams(BaseModel):\n",
    "        \"\"\"Parameters for analyze_peak_periods function\"\"\"\n",
    "        start_date: str | None = None\n",
    "        end_date: str | None = None\n",
    "        day_type: str | None = None\n",
    "        hour_range: List[int] | None = None  # Changed from Tuple to List\n",
    "        time_period: str | None = None\n",
    "        vehicle_class: str | None = None\n",
    "        entry_point: str | None = None\n",
    "        entry_region: str | None = None\n",
    "        granularity: str | None = \"hour\"\n",
    "        top_n: int | None = 5\n",
    "    \n",
    "    class AnalyzeVehicleDistributionParams(BaseModel):\n",
    "        \"\"\"Parameters for analyze_vehicle_distribution function\"\"\"\n",
    "        start_date: str | None = None\n",
    "        end_date: str | None = None\n",
    "        day_type: str | None = None\n",
    "        hour_range: List[int] | None = None  # Changed from Tuple to List\n",
    "        time_period: str | None = None\n",
    "        vehicle_class: str | None = None\n",
    "        entry_point: str | None = None\n",
    "        entry_region: str | None = None\n",
    "        compare_with: Dict[str, Any] | None = None\n",
    "    \n",
    "    class AnalyzeTimeTrendsParams(BaseModel):\n",
    "        \"\"\"Parameters for analyze_time_trends function\"\"\"\n",
    "        start_date: str | None = None\n",
    "        end_date: str | None = None\n",
    "        day_type: str | None = None\n",
    "        hour_range: List[int] | None = None  # Changed from Tuple to List\n",
    "        time_period: str | None = None\n",
    "        vehicle_class: str | None = None\n",
    "        entry_point: str | None = None\n",
    "        entry_region: str | None = None\n",
    "        metric: str | None = \"CRZ Entries\"\n",
    "        time_unit: str | None = \"day\"\n",
    "    \n",
    "    class AnalyzeExcludedRoadwayUsageParams(BaseModel):\n",
    "        \"\"\"Parameters for analyze_excluded_roadway_usage function\"\"\"\n",
    "        start_date: str | None = None\n",
    "        end_date: str | None = None\n",
    "        day_type: str | None = None\n",
    "        hour_range: List[int] | None = None  # Changed from Tuple to List\n",
    "        time_period: str | None = None\n",
    "        vehicle_class: str | None = None\n",
    "        entry_point: str | None = None\n",
    "        entry_region: str | None = None\n",
    "    \n",
    "    class AnalyzeVehiclePatternsParams(BaseModel):\n",
    "        \"\"\"Parameters for analyze_vehicle_patterns function\"\"\"\n",
    "        start_date: str | None = None\n",
    "        end_date: str | None = None\n",
    "        day_type: str | None = None\n",
    "        hour_range: List[int] | None = None  # Changed from Tuple to List\n",
    "        time_period: str | None = None\n",
    "        vehicle_class: str  # Required field\n",
    "        entry_point: str | None = None\n",
    "        entry_region: str | None = None\n",
    "    \n",
    "    class CompareTrafficSegmentsParams(BaseModel):\n",
    "        \"\"\"Parameters for compare_traffic_segments function\"\"\"\n",
    "        dimension: str  # Required field\n",
    "        segment_a: Dict[str, Any]  # Required field\n",
    "        segment_b: Dict[str, Any]  # Required field\n",
    "        metric: str | None = \"CRZ Entries\"\n",
    "    \n",
    "    class GenerateVisualizationParams(BaseModel):\n",
    "        \"\"\"Parameters for generate_visualization function\"\"\"\n",
    "        start_date: str | None = None\n",
    "        end_date: str | None = None\n",
    "        day_type: str | None = None\n",
    "        hour_range: List[int] | None = None  # Changed from Tuple to List\n",
    "        time_period: str | None = None\n",
    "        vehicle_class: str | None = None\n",
    "        entry_point: str | None = None\n",
    "        entry_region: str | None = None\n",
    "        chart_type: str  # Required field\n",
    "        x_column: str  # Required field\n",
    "        y_column: str  # Required field\n",
    "        title: str | None = \"Congestion Relief Zone Analysis\"\n",
    "\n",
    "\n",
    "def get_params_model(function_name: str) -> Type[BaseModel]:\n",
    "    \"\"\"\n",
    "    Returns the appropriate Pydantic model based on the function name\n",
    "    \n",
    "    Args:\n",
    "        function_name: Name of the function\n",
    "        \n",
    "    Returns:\n",
    "        The corresponding Pydantic model class\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the function name is not recognized\n",
    "    \"\"\"\n",
    "    model_mapping = {\n",
    "        \"filter_crz_data\": FunctionParams.FilterCRZDataParams,\n",
    "        \"analyze_entry_point_volume\": FunctionParams.AnalyzeEntryPointVolumeParams,\n",
    "        \"analyze_peak_periods\": FunctionParams.AnalyzePeakPeriodsParams,\n",
    "        \"analyze_vehicle_distribution\": FunctionParams.AnalyzeVehicleDistributionParams,\n",
    "        \"analyze_time_trends\": FunctionParams.AnalyzeTimeTrendsParams,\n",
    "        \"analyze_excluded_roadway_usage\": FunctionParams.AnalyzeExcludedRoadwayUsageParams,\n",
    "        \"analyze_vehicle_patterns\": FunctionParams.AnalyzeVehiclePatternsParams,\n",
    "        \"compare_traffic_segments\": FunctionParams.CompareTrafficSegmentsParams,\n",
    "        \"generate_visualization\": FunctionParams.GenerateVisualizationParams\n",
    "    }\n",
    "    \n",
    "    if function_name not in model_mapping:\n",
    "        raise ValueError(f\"Unknown function name: {function_name}\")\n",
    "    \n",
    "    return model_mapping[function_name]\n",
    "\n",
    "\n",
    "def convert_list_to_tuple(params_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Convert lists to tuples in the parameters dictionary where needed\n",
    "    \n",
    "    Args:\n",
    "        params_dict: Dictionary of parameters from LLM response\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with lists converted to tuples where appropriate\n",
    "    \"\"\"\n",
    "    result = params_dict.model_copy()\n",
    "    \n",
    "    # Convert hour_range from list to tuple if it exists and is a list\n",
    "    if 'hour_range' in result and result['hour_range'] is not None:\n",
    "        if isinstance(result['hour_range'], list) and len(result['hour_range']) == 2:\n",
    "            result['hour_range'] = tuple(result['hour_range'])\n",
    "    \n",
    "    # Handle nested dictionaries like segment_a and segment_b\n",
    "    for key in ['segment_a', 'segment_b', 'compare_with']:\n",
    "        if key in result and isinstance(result[key], dict):\n",
    "            result[key] = convert_list_to_tuple(result[key])\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_crz_function(function_name: str, params: BaseModel, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Execute the appropriate CRZ analysis function based on the function name and parameters\n",
    "    \n",
    "    Args:\n",
    "        function_name: Name of the function to call\n",
    "        params: Pydantic model instance containing the function parameters\n",
    "        df: DataFrame containing the CRZ data\n",
    "        \n",
    "    Returns:\n",
    "        Result of the function call\n",
    "    \"\"\"\n",
    "    # Convert Pydantic model to dictionary\n",
    "    params_dict = params.model_dump(exclude_none=True)\n",
    "    \n",
    "    # Handle hour_range conversion from list to tuple if needed\n",
    "    if 'hour_range' in params_dict and isinstance(params_dict['hour_range'], list):\n",
    "        params_dict['hour_range'] = tuple(params_dict['hour_range'])\n",
    "    \n",
    "    # Map function names to their implementations\n",
    "    function_mapping = {\n",
    "        \"filter_crz_data\": filter_crz_data,\n",
    "        \"analyze_entry_point_volume\": analyze_entry_point_volume,\n",
    "        \"analyze_peak_periods\": analyze_peak_periods, \n",
    "        \"analyze_vehicle_distribution\": analyze_vehicle_distribution,\n",
    "        \"analyze_time_trends\": analyze_time_trends,\n",
    "        \"analyze_excluded_roadway_usage\": analyze_excluded_roadway_usage,\n",
    "        #\"analyze_vehicle_patterns\": analyze_vehicle_patterns,\n",
    "        \"compare_traffic_segments\": compare_traffic_segments,\n",
    "    }\n",
    "    \n",
    "    # Verify function exists\n",
    "    if function_name not in function_mapping:\n",
    "        raise ValueError(f\"Unknown function: {function_name}\")\n",
    "    \n",
    "    # Get the function\n",
    "    func = function_mapping[function_name]\n",
    "    \n",
    "    # Special handling for compare_traffic_segments which has nested parameters\n",
    "    if function_name == \"compare_traffic_segments\":\n",
    "        # Need to process nested segment_a and segment_b dictionaries\n",
    "        if 'segment_a' in params_dict:\n",
    "            for key, value in params_dict['segment_a'].items():\n",
    "                if key == 'hour_range' and isinstance(value, list):\n",
    "                    params_dict['segment_a'][key] = tuple(value)\n",
    "                    \n",
    "        if 'segment_b' in params_dict:\n",
    "            for key, value in params_dict['segment_b'].items():\n",
    "                if key == 'hour_range' and isinstance(value, list):\n",
    "                    params_dict['segment_b'][key] = tuple(value)\n",
    "    \n",
    "    # Call the function with the dataframe and parameters\n",
    "    try:\n",
    "        # Always pass the dataframe as the first argument\n",
    "        result = func(df, **params_dict)\n",
    "        return result\n",
    "    except TypeError as e:\n",
    "        # If we get a TypeError, it might be due to unexpected parameters\n",
    "        # Log the error and re-raise with more helpful message\n",
    "        print(f\"Error calling {function_name}: {e}\")\n",
    "        print(f\"Parameters provided: {params_dict}\")\n",
    "        raise ValueError(f\"Error calling {function_name} with the provided parameters: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State definitions\n",
    "class Reception(BaseModel):\n",
    "    \"\"\"Schema for understanding user queries about Congestion Relief Zone data\"\"\"\n",
    "    response: str = Field(\n",
    "        description=\"The response to the user's query, in a concise and informative manner\"\n",
    "    )\n",
    "    retrieve_data: bool = Field(\n",
    "        description=\"do we need to retrieve data from the dataset to answer the user's query\"\n",
    "    )\n",
    "    data_description: str = Field(\n",
    "        description=\"a description of the data needed to be retrieved from the dataset to answer the user's query\"\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "class FindFunction(BaseModel):\n",
    "    response: str = Field(\n",
    "        description=\"The response to the user's query, in a concise and informative manner\"\n",
    "    )\n",
    "    data_available: bool = Field(\n",
    "        description=\"do we have the data needed to answer the user's query\"\n",
    "    )\n",
    "    function_name: str = Field(\n",
    "        description=\"the name of the function to call to retrieve the data\"\n",
    "    )\n",
    "    \n",
    "class FinalAnswer(BaseModel):\n",
    "    response: str = Field(\n",
    "        description=\"The response to the user's query, in a concise and informative manner\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reception_agent(query: str) -> Reception:\n",
    "    reception_template = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"You are data analyst specializing in NYC Congestion Relief Zone data.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\n",
    "                    \"human\",\n",
    "                    \"\"\"You are given a user's query.{query} You need to first determine if you need to retrieve data from the dataset to answer the user's query.\n",
    "                    If you need to retrieve data, you need to describe the data needed to be retrieved, you should provide response as retrieving data from the dataset.\n",
    "                    If you don't need to retrieve data, you need to provide a response to the user's query.\n",
    "                    response: The response to the user's query, in a concise and informative manner\n",
    "                    retrieve_data: do we need to retrieve data from the dataset to answer the user's query\n",
    "                    data_description: a description of the data needed to be retrieved from the dataset to answer the user's query\n",
    "                    \"\"\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    prompt = reception_template.format(query=query)\n",
    "    model_name = \"gemini-2.0-flash\"\n",
    "    model_provider = \"Gemini\"\n",
    "    pydantic_model = Reception\n",
    "    max_retries = 3\n",
    "    return call_llm(prompt, model_name, model_provider, pydantic_model, max_retries)\n",
    "\n",
    "def function_selection_agent(user_query: str, data_description: str, functions: json) -> Reception:\n",
    "    function_selection_template = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"You are data analyst specializing in NYC Congestion Relief Zone data.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\n",
    "                    \"human\",\n",
    "                    \"\"\"You are given a task to retrive data from NYC Congestion Relief Zone data. User is asking for {user_query} \n",
    "                    To answer the user's query, you need to retrieve data that is described as {data_description}\n",
    "                    You need to first determine if one of the functions in the following list can be used to retrieve the data:\n",
    "                    {functions}\n",
    "                    If yes, you need to provide the name of the function to call to retrieve the data.\n",
    "                    If no, you need to provide a response to the user's query, informing the user that the data is not available in the dataset.\n",
    "                    response: The response to the user's query, in a concise and informative manner\n",
    "                    data_available: do we have the data needed to answer the user's query\n",
    "                    function_name: the name of the function to call to retrieve the data\n",
    "                    \n",
    "                    \"\"\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    prompt = function_selection_template.format(user_query=user_query, data_description=data_description, functions=functions)\n",
    "    model_name = \"gemini-2.0-flash\"\n",
    "    model_provider = \"Gemini\"\n",
    "    pydantic_model = FindFunction\n",
    "    print(pydantic_model)\n",
    "    print(type(pydantic_model))\n",
    "    max_retries = 3\n",
    "    return call_llm(prompt, model_name, model_provider, pydantic_model, max_retries)\n",
    "\n",
    "def data_retrieval_agent(user_query: str, function_name: str, functions_description: json):\n",
    "    functions_description = json.dumps(functions_description)\n",
    "    data_retrieval_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are data analyst specializing in NYC Congestion Relief Zone data.\n",
    "                \"\"\",\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"\"\"You are given a task to retrieve data from NYC Congestion Relief Zone data. User is asking for {user_query} \n",
    "                To answer the user's query, you need to retrieve data that is described as {functions_description}\n",
    "                You need to provide the parameters to pass to the function to retrieve the data.\n",
    "                response: The response to the user's query, in a concise and informative manner\n",
    "                function_params: the parameters to pass to the function to retrieve the data\n",
    "                \"\"\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    prompt = data_retrieval_template.format(user_query=user_query, functions_description=functions_description)\n",
    "    model_name = \"gemini-2.0-flash\"\n",
    "    model_provider = \"Gemini\"\n",
    "    pydantic_model = get_params_model(function_name)\n",
    "    max_retries = 3\n",
    "    return call_llm(prompt, model_name, model_provider, pydantic_model, max_retries)\n",
    "\n",
    "def final_answer_agent(user_query: str, retrieved_data: Any):\n",
    "    final_answer_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are data analyst specializing in NYC Congestion Relief Zone data.\n",
    "                \"\"\",\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"\"\"You are given a task to retrieve data from NYC Congestion Relief Zone data. User is asking for {user_query} \n",
    "                To answer the user's query, you have retrieved the following data: {retrieved_data}\n",
    "                \"\"\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    prompt = final_answer_template.format(user_query=user_query, retrieved_data=retrieved_data)\n",
    "    model_name = \"gemini-2.0-flash\"\n",
    "    model_provider = \"Gemini\"\n",
    "    pydantic_model = FinalAnswer\n",
    "    max_retries = 3\n",
    "    return call_llm(prompt, model_name, model_provider, pydantic_model, max_retries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is the busiest entry point in the dataset?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reception_response = reception_agent(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(reception_response.data_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic volume data for each entry point into the Congestion Relief Zone, including timestamps.\n",
      "<class '__main__.FindFunction'>\n",
      "<class 'pydantic._internal._model_construction.ModelMetaclass'>\n",
      "analyze_entry_point_volume\n",
      "{'function_name': 'analyze_entry_point_volume', 'description': 'Analyze traffic volumes for different entry points', 'required_parameters': [], 'optional_parameters': ['start_date', 'end_date', 'day_type', 'hour_range', 'time_period', 'vehicle_class', 'top_n', 'include_excluded_roadways'], 'returns': 'Dictionary with top entry points by volume and percentage'}\n"
     ]
    }
   ],
   "source": [
    "if reception_response.retrieve_data:\n",
    "    print(reception_response.data_description)\n",
    "    function_selection_response = function_selection_agent(user_query=user_query, data_description=reception_response.data_description, functions=functions)\n",
    "    if function_selection_response.data_available:\n",
    "        print(function_selection_response.function_name)\n",
    "        function_description = find_function_description(function_selection_response.function_name)\n",
    "        print(function_description)\n",
    "    else:\n",
    "        print(function_selection_response.response)\n",
    "else:\n",
    "    print(reception_response.response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_date=None end_date=None day_type=None hour_range=None time_period=None vehicle_class=None entry_point=None entry_region=None top_n=1 include_excluded_roadways=False\n"
     ]
    }
   ],
   "source": [
    "data_retrieval_response = data_retrieval_agent(user_query=user_query, function_name=function_selection_response.function_name, functions_description=function_description)\n",
    "print(data_retrieval_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = execute_crz_function(function_name=function_selection_response.function_name, params=data_retrieval_response, df=  df_mta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'top_entry_points': [{'entry_point': 'East 60th St', 'region': 'East 60th St', 'volume': 6366481, 'percentage': 15.7}], 'total_volume': 40495859, 'filter_summary': {'date_range': '2025-01-05 to 2025-03-29', 'day_type': 'All days', 'hour_range': 'All hours', 'time_period': 'All periods', 'vehicle_class': 'All vehicles', 'entry_count': 12}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response='The busiest entry point is East 60th St with a volume of 6,366,481 vehicles, representing 15.7% of the total volume in the dataset.'\n"
     ]
    }
   ],
   "source": [
    "final_answer_response = final_answer_agent(user_query=user_query, retrieved_data=result)\n",
    "print(final_answer_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocksflags",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
